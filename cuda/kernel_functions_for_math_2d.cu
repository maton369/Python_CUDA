/*
 * 2次元配列（ny 行 × nx 列）を要素ごとに加算する CUDA グローバル関数（カーネル）である。
 * 目的: output[y, x] = arr1[y, x] + arr2[y, x] を全要素について並列に実行する。
 *
 * 実行モデルの要点:
 * - 本カーネルは 2D グリッド × 2D ブロック構成を前提としており、各スレッドが一意の (x, y) を担当する。
 * - スレッドは線形メモリ上のインデックス ij = nx * y + x を介して配列へアクセスする。
 * - 範囲外のスレッド（グリッドが配列境界を越えて起動された場合）を if ガードで安全に排除する。
 *
 * 引数:
 *   nx, ny   : 配列の幅（列数）と高さ（行数）である。合計要素数は nx * ny。
 *   output   : 出力配列（デバイスメモリ上の float*）。arr1 + arr2 の和を書き込む。
 *   arr1,arr2: 入力配列（デバイスメモリ上の float*）。いずれも同じ形状(nx*ny)を仮定する。
 *
 * メモリアクセスの観点:
 * - 行優先（row-major）な線形化: ij = nx * y + x により、x が連続する要素はアドレス連続となる。
 * - blockDim.x を warp 幅（32）の倍数にし、threadIdx.x が x に対応する設計とすることで、
 *   同一行内のアクセスがコアレス化（coalesced）されやすく、帯域効率が高まる。
 * - y 方向は行の切り替えに相当し、行境界を跨ぐ際はメモリアクセスの連続性が低下しうる。
 *
 * 実装のポイント:
 * - 2D の (blockIdx.{x,y}, threadIdx.{x,y}) から (x,y) を計算する。
 * - 範囲チェック (x<nx && y<ny) によって、端数ブロックでの越境アクセスを防止する。
 * - float 演算は単純な要素加算であり、メモリ帯域がボトルネックになりやすい（メモリバウンド）。
 *   性能評価ではブロック形状（例: 32×8, 16×16 等）を変えて帯域活用を最適化するとよい。
 */
__global__ void add_two_array_kernel(int nx, int ny, float *output, float *arr1, float *arr2){
    // 各スレッドに割り当てられる 2D 座標 (x, y) を計算するである。
    // threadIdx.{x,y} : ブロック内のスレッド座標
    // blockDim.{x,y}  : ブロックあたりのスレッド数
    // blockIdx.{x,y}  : グリッド内のブロック座標
    // → x は列方向、y は行方向のグローバル座標を表す。
    const int x = threadIdx.x + blockDim.x * blockIdx.x;
    const int y = threadIdx.y + blockDim.y * blockIdx.y;

    // 行優先（row-major）で 2D → 1D の線形インデックスに変換するである。
    // 1 行あたり nx 要素であるため、先頭から y 行分のオフセット nx*y に x を足す。
    int ij = nx * y + x;

    // 範囲外アクセスを防ぐガードである。
    // グリッドはしばしば「端数を含む大きめ」に起動するため、
    // 配列境界チェックは必須である。
    if (x < nx && y < ny){
        // 要素単位の加算を実行するである。
        // output[ij] へ arr1[ij] + arr2[ij] の和を書き込む。
        // x が連続するスレッドが同一行を担当していれば、読み書きは coalesced になりやすい。
        output[ij] = arr1[ij] + arr2[ij];
    }
}

/*
 * 補足（最適化の考え方）:
 * - ブロック形状: 例として blockDim=(32,8) など、x 方向を 32 の倍数にして coalesced を促進する。
 * - メモリアクセス: 読み込み 2 配列 + 書き込み 1 配列で合計 3*4B = 12B/要素。算術密度が低く帯域律速になりやすい。
 * - 単精度 FMA などの演算を伴わないため、基本的にメモリ帯域の最大化（Occupancy とアクセスパターン最適化）が肝要である。
 * - 大規模入力ではストリームや共有メモリ最適化は不要（単純加算）だが、タイル化で L2/L1 キャッシュ効率が改善する可能性はある。
 */